{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitanaconda3virtualenv3e8f700443744979aab46604b31bab53",
   "display_name": "Python 3.7.6 64-bit ('anaconda3': virtualenv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from utilities import visualize_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input data\n",
    "input_file = 'data_random_forests.txt'\n",
    "data = np.loadtxt(input_file, delimiter=',')\n",
    "X, y = data[:, :-1], data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate input data into three classes based on labels\n",
    "class_0 = np.array(X[y==0])\n",
    "class_1 = np.array(X[y==1])\n",
    "class_2 = np.array(X[y==2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing datasets\n",
    "# X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.25, random_state=5)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "parameter_grid = [ {'n_estimators': [100], 'max_depth': [2, 4, 7, 12, 16]},\n",
    "                   {'max_depth': [4], 'n_estimators': [25, 50, 100, 250]}\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['precision_weighted', 'recall_weighted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "##### Searching optimal parameters for precision_weighted\n",
      "\n",
      "Grid scores for the parameter grid:\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'grid_scores_'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-733fb4b18ca3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nGrid scores for the parameter grid:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-->'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'grid_scores_'"
     ]
    }
   ],
   "source": [
    "for metric in metrics:\n",
    "    print(\"\\n##### Searching optimal parameters for\", metric)\n",
    "\n",
    "    classifier = GridSearchCV(ExtraTreesClassifier(random_state=0), parameter_grid, cv=5, scoring=metric)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # print(\"\\nGrid scores for the parameter grid:\")\n",
    "    # for params, avg_score, _ in classifier.grid_scores_:\n",
    "    #     print(params, '-->', round(avg_score, 3))\n",
    "\n",
    "    # print(\"\\nBest parameters:\", classifier.best_params_)\n",
    "\n",
    "    # y_pred = classifier.predict(X_test)\n",
    "    # print(\"\\nPerformance report:\\n\")\n",
    "    # print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "source": [
    "## Menggunakan Library Pandas\n",
    "https://scikit-learn.org/dev/whats_new/v0.18.html?highlight=grid_search%20gridsearchcv"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(classifier.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   mean_score_time  std_score_time param_max_depth param_n_estimators  \\\n0         0.014613        0.002833               2                100   \n1         0.014705        0.001210               4                100   \n2         0.013826        0.000427               7                100   \n3         0.015044        0.000386              12                100   \n4         0.015155        0.000218              16                100   \n5         0.004865        0.000393               4                 25   \n6         0.007764        0.000180               4                 50   \n7         0.013539        0.000166               4                100   \n8         0.031414        0.001682               4                250   \n\n                                   params  split0_test_score  \\\n0   {'max_depth': 2, 'n_estimators': 100}           0.874603   \n1   {'max_depth': 4, 'n_estimators': 100}           0.853234   \n2   {'max_depth': 7, 'n_estimators': 100}           0.853813   \n3  {'max_depth': 12, 'n_estimators': 100}           0.815545   \n4  {'max_depth': 16, 'n_estimators': 100}           0.800374   \n5    {'max_depth': 4, 'n_estimators': 25}           0.875586   \n6    {'max_depth': 4, 'n_estimators': 50}           0.845126   \n7   {'max_depth': 4, 'n_estimators': 100}           0.853234   \n8   {'max_depth': 4, 'n_estimators': 250}           0.860670   \n\n   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n0           0.876943           0.819569           0.820784           0.856884   \n1           0.867377           0.828341           0.802601           0.854124   \n2           0.876627           0.826111           0.801926           0.860624   \n3           0.876517           0.810303           0.803514           0.853896   \n4           0.851904           0.775697           0.794215           0.860232   \n5           0.855950           0.834651           0.793105           0.868577   \n6           0.850352           0.837845           0.802601           0.863329   \n7           0.867377           0.828341           0.802601           0.854124   \n8           0.878879           0.828341           0.801926           0.854124   \n\n   mean_test_score  std_test_score  rank_test_score  \n0         0.849757        0.025132                1  \n1         0.841135        0.023032                5  \n2         0.843820        0.026560                4  \n3         0.831955        0.028334                8  \n4         0.816484        0.033429                9  \n5         0.845574        0.029698                2  \n6         0.839851        0.020401                7  \n7         0.841135        0.023032                5  \n8         0.844788        0.026867                3  \n"
     ]
    }
   ],
   "source": [
    "df_columns_to_print = [column for column in df.columns if 'param' in column or 'score' in column]\n",
    "print(df[df_columns_to_print])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}